{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project: Data Analysis of Ford GoBike Dataset\n",
    "### By: Abebe Tarekegne\n",
    "### Published: on May 10, 2019\n",
    "\n",
    "\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">Ford GoBike is the Bay Area's new bike share system, with thousands of public bikes for use across San Francisco, East Bay and San Jose. It involves a fleet of specially designed bikes that are locked into a network of docking stations. Bikes can be unlocked from one station and returned to any other station in the system. People use bike share to commute to work or school, run errands, get to appointments, and more. \n",
    "\n",
    ">We are going to investigate publicly available datasets to help predict growth of bike ride, or if the use of bike ride differs by city, age, gender, day of the week, hour of the day, weekend.\n",
    "\n",
    ">The begining section shows how the dataset are gathered and wrangled but one can go directly to:\n",
    "- Exploratory section from [here](#exploratory)\n",
    "- Explanatory section from [here](#explanatory)\n",
    "- Conclusion section from [here](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminary Wrangling\n",
    "\n",
    ">We use Ford GoBike's trip Data available for public use [here](https://www.fordgobike.com/system-data)\n",
    "\n",
    ">`The attributes includes:`\n",
    ">- trip duration,\n",
    ">- start/end time and date, \n",
    ">- start/end station id, name, lat/long, \n",
    ">- bike id, \n",
    ">- user type\n",
    ">- member year of birth, and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "import pandas as pd    # For database management\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import csv\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "timer_dict = {}\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Download Datasets (.csv and .zip)\n",
    "\n",
    ">Looking to the list of downloadable files from [here](https://www.fordgobike.com/system-data), it seems that the data available are one csv file from 2017 and the rest are from 2018 and 2019 and all are csv but zip files. I show how promgramatically download those files. \n",
    "\n",
    ">First new folder was created and ready to use, then I code to capture both format of the file (.csv and zipped csv) from the link provided.  I used the list created by looking the file names which contains unique numbers from part of the file names itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Make directory in the local working directory if it doesn't already exist\n",
    "folder_name = 'fordgobike_files'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "#list of unique numbers from filenames\n",
    "f_lst = list(range(201801,201813)) + [2017, 201901, 201902, 201903]\n",
    "\n",
    "for f_num in f_lst:\n",
    "    f_num = str(f_num) #convert integers to string\n",
    "    if f_num == \"2017\": # non-zip .csv files\n",
    "        f_url = 'https://s3.amazonaws.com/fordgobike-data/'+ f_num + '-fordgobike-tripdata.csv'\n",
    "        temp_file_name = f_num + '-fordgobike-tripdata.csv'\n",
    "        download = requests.get(f_url,stream=True)    # returns type 'str'\n",
    "        with open(folder_name +'\\\\'+ temp_file_name, 'w') as temp_file: # str, hence mode 'w'\n",
    "            temp_file.write(download.text)\n",
    "    else:\n",
    "        f_url = 'https://s3.amazonaws.com/fordgobike-data/'+ f_num + '-fordgobike-tripdata.csv.zip'\n",
    "        r = requests.get(f_url,stream=True) # check for <Response [200]> good\n",
    "        check = zipfile.is_zipfile(io.BytesIO(r.content)) \n",
    "        if check: # check if zip file and ignore for other formats\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create Master DataFrame\n",
    "\n",
    ">Now all .csv and .zip files are downloaded to the specified folder name and zip files are unzipped so that all files are in the same format \".csv\". \n",
    "\n",
    ">We imported iglob from [glob](https://docs.python.org/3/library/glob.html) python module in order to use recursive list comprehension to each files in a given path. [Examples](https://stackoverflow.com/questions/2186525/how-to-use-glob-to-find-files-recursively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from glob import iglob\n",
    "import sys\n",
    "\n",
    "path = r'C:\\Users\\at9490\\Documents\\IPython Notebooks\\DataAnalyst\\project4\\fordgobike_files\\*.csv' #path\n",
    "# after reading each csv file and concatenate all assigned to pandas dataframe.\n",
    "start = timer()\n",
    "df_master = pd.concat((pd.read_csv(f) for f in iglob(path, recursive=True)), ignore_index=True, sort=False)\n",
    "end = timer()\n",
    "timeittake = end - start\n",
    "timer_dict.update({\"csv concat\":timeittake})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows:  3015214\n",
      "Number of Columns:  16\n"
     ]
    }
   ],
   "source": [
    "# high-level overview of data shape and composition\n",
    "print(\"Number of Rows: \",df_master.shape[0])\n",
    "print(\"Number of Columns: \",df_master.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <center>Original Master Dataset Features</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| `Feature Name`          | `Data Type` | `Unique Count` | `NaN Count` |\n",
    "|-------------------------|-----------|--------------|-----------|\n",
    "| duration_sec            | int64     | 22152        | 0         |\n",
    "| start_time              | object    | 3015020      | 0         |\n",
    "| end_time                | object    | 3015038      | 0         |\n",
    "| start_station_id        | float64   | 360          | 12437     |\n",
    "| start_station_name      | object    | 381          | 12437     |\n",
    "| start_station_latitude  | float64   | 407          | 0         |\n",
    "| start_station_longitude | float64   | 408          | 0         |\n",
    "| end_station_id          | float64   | 360          | 12437     |\n",
    "| end_station_name        | object    | 381          | 12437     |\n",
    "| end_station_latitude    | float64   | 408          | 0         |\n",
    "| end_station_longitude   | float64   | 410          | 0         |\n",
    "| bike_id                 | int64     | 6802         | 0         |\n",
    "| user_type               | object    | 2            | 0         |\n",
    "| member_birth_year       | float64   | 92           | 206967    |\n",
    "| member_gender           | object    | 3            | 206534    |\n",
    "| bike_share_for_all_trip | object    | 2            | 519700    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates:  0\n",
      "\n",
      "User Type: Number of rides\n",
      "Subscriber    2544383\n",
      "Customer       470831\n",
      "Name: user_type, dtype: int64\n",
      "\n",
      "Bike Share For All Trip: Number of rides\n",
      "No     2277409\n",
      "Yes     218105\n",
      "Name: bike_share_for_all_trip, dtype: int64\n",
      "\n",
      "Member Gender: Number of rides\n",
      "Male      2082988\n",
      "Female     680192\n",
      "Other       45500\n",
      "Name: member_gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates: ',df_master.duplicated().sum())\n",
    "print()\n",
    "print('User Type: Number of rides')\n",
    "print(df_master.user_type.value_counts())\n",
    "print()\n",
    "print('Bike Share For All Trip: Number of rides')\n",
    "print(df_master.bike_share_for_all_trip.value_counts())\n",
    "print()\n",
    "print('Member Gender: Number of rides')\n",
    "print(df_master.member_gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">`data type convert:` to approprate datatype \n",
    ">- 'start_time' str to datetime\n",
    ">- 'end_time' str to datetime \n",
    ">- 'user_type' str to category \n",
    ">- 'member_gender' str to category \n",
    ">- 'bike_share_for_all_trip' str to category \n",
    ">- 'start_station_id' float to int\n",
    ">- 'end_station_id' float to int\n",
    ">- 'member_birth_year' float to datetime\n",
    "\n",
    ">`...more`\n",
    ">- catagorical variables: 'user_type' and 'member_gender'\n",
    ">- datetime variables: 'start_time' and 'end_time'\n",
    ">- integer variables: 'start_station_id' and 'end_station_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    ">Station id, station names, station lat/long we expect to see the same number of unique counts but there were few records with more than one lat/long locations for the same station ID. The same is true for names, as in some cases temporary names were used. For our visualization we will use `station_id` as the bases for referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of original DataFrame\n",
    "df_clean = df_master.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Convert from object to category\n",
    "df_clean['user_type'] = df_clean['user_type'].astype('category')\n",
    "df_clean['member_gender'] = df_clean['member_gender'].astype('category')\n",
    "df_clean['bike_share_for_all_trip'] = df_clean['bike_share_for_all_trip'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Convert objects to datetime\n",
    "df_clean['start_time']  = pd.to_datetime(df_clean['start_time'])\n",
    "df_clean['end_time']  = pd.to_datetime(df_clean['end_time'])\n",
    "\n",
    "#Using datetime I created additional time features\n",
    "df_clean['start_hour'] = df_clean['start_time'].dt.hour #0 to 22\n",
    "df_clean['start_date'] = df_clean['start_time'].dt.date \n",
    "df_clean['start_weekday'] = df_clean['start_time'].dt.weekday #0 to 6\n",
    "df_clean['start_day'] = df_clean['start_time'].dt.day\n",
    "df_clean['start_month'] = df_clean['start_time'].dt.month #0 to ..\n",
    "df_clean['start_year'] = df_clean['start_time'].dt.year\n",
    "df_clean['start_weekofyear'] = df_clean['start_time'].dt.weekofyear  # 0 to 52\n",
    "df_clean['start_weekday_name'] = df_clean['start_time'].dt.strftime('%a') # %a Sun %A Sunday\n",
    "df_clean['start_month_name'] = df_clean['start_time'].dt.strftime('%b') # %b Jan %B January\n",
    "df_clean['start_year_month'] = df_clean['start_time'].dt.strftime('%y-%m') # %y 17 or  %Y 2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#After replacing NaN values with 0 I convert those listed fields to integers\n",
    "df_clean[['start_station_id', 'end_station_id','member_birth_year']] = \\\n",
    "df_clean[list(['start_station_id', 'end_station_id','member_birth_year'])].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3015214, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start_station_id     0\n",
       "end_station_id       0\n",
       "member_birth_year    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[['start_station_id', 'end_station_id','member_birth_year']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the structure of the dataset?\n",
    "\n",
    ">There are 3015214 attributes (rides recorded) with 16 original features, and created 9 new features of dates to help for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the main feature(s) of interest in the dataset?\n",
    ">I am most interested in the `duration_sec` but as the start and end oordinates are provided it will be more interesting to find the distance or the average speed of each trip so `new features will be added`.\n",
    "\n",
    ">And Interesting to investigate the trip duration relationship with:\n",
    ">- User Type (Subscriber or Customer – “Subscriber” = Member or “Customer” = Casual)\n",
    ">- Member Year of Birth\n",
    ">- Member Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What type of new features are introduced?\n",
    "\n",
    ">- Trip Distance\n",
    ">- Group Station Locations by Cluster of Areas or Cities\n",
    ">- Member Age Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Trip Distance Calcuation\n",
    "\n",
    ">Since trip start and end locations are provided I am interested to find the minimum possible distances. With time and distance then we can drive speed.\n",
    "\n",
    ">We are now creating new feature called `minimum_distance_miles` and `average_speed_mph`\n",
    "\n",
    ">Let's import geopy module to use for spatial data manuplation. Geopy can calculate geodesic distance between two points using the geodesic distance or the great-circle distance, with a default of the geodesic distance available as the function geopy.distance.distance.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def distance(origin, destination):\n",
    "    dist = geopy.distance.geodesic(origin, destination).miles\n",
    "    return dist\n",
    "\n",
    "start = timer() #this may take up to 20minutes\n",
    "# minimum distance traveled from the start to end of the trip\n",
    "df_clean['minimum_distance_miles'] = df_clean.apply(lambda x: distance((x['start_station_latitude'], x['start_station_longitude']), \\\n",
    "                                                               (x['end_station_latitude'], x['end_station_longitude'])), axis=1)\n",
    "end = timer()\n",
    "timeittake = end - start\n",
    "timer_dict.update({\"distance\":timeittake})\n",
    "\n",
    "# trip speed in miles per hour\n",
    "df_clean['duration_hr'] = df_clean['duration_sec']/3600\n",
    "df_clean['average_speed_mph'] = df_clean['minimum_distance_miles']/df_clean['duration_hr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Group Station Locations by Cluster of Areas or Cities\n",
    "\n",
    ">We now create new feature called `city` based on lat/long filtering using:\n",
    "\n",
    ">- `San Francisco` longitude range(-122.48,-122.35) and latitude range(37.73,37.83)\n",
    ">- `Oakland` longitude range(-122.32,-122.2) and latitude range(37.77,37.88)\n",
    ">- `San Jose` longitude range(-122,-121.8) and latitude range(37.3,37.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "san_index = df_clean.query('(start_station_longitude > -122.48) & (start_station_longitude < -122.35) & \\\n",
    "(start_station_latitude > 37.73) & (start_station_latitude < 37.83)').index\n",
    "oak_index = df_clean.query('(start_station_longitude > -122.32) & (start_station_longitude < -122.2) & \\\n",
    "     (start_station_latitude > 37.77) & (start_station_latitude < 37.88)').index\n",
    "saj_index = df_clean.query('(start_station_longitude > -122) & (start_station_longitude < -121.8) & \\\n",
    "     (start_station_latitude > 37.3) & (start_station_latitude < 37.5)').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City:    Number of rides\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "San Francisco    2240531\n",
       "Oakland           625782\n",
       "San Jose          143945\n",
       "Other               4956\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['city']=\"\" # create empty column\n",
    "df_clean.loc[san_index,'city'] = \"San Francisco\"\n",
    "df_clean.loc[oak_index,'city'] = \"Oakland\"\n",
    "df_clean.loc[saj_index,'city'] = \"San Jose\"\n",
    "# We append all three cities indexes and substract from dataset indexes to get for other part of town\n",
    "noncity_index = list(set(df_clean.index)-set(san_index.append(oak_index).append(saj_index)))\n",
    "df_clean.loc[noncity_index,'city'] = \"Other\"\n",
    "df_clean['city'] = df_clean['city'].astype('category')\n",
    "print('City:    Number of rides')\n",
    "df_clean['city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Member Age Group\n",
    "\n",
    ">We are now creating new feature called `member_age_group` which shows grouping based on the member age range or bins we created. From member_birth_year we use 2019 as reference to find the member_age, and then we use bining to divide in groups of managable size for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group: Number of rides\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-20        33381\n",
       "20-30       926499\n",
       "30-40      1096387\n",
       "40-50       443560\n",
       "50-60       235526\n",
       "60-70        60494\n",
       "70-100       10579\n",
       "100-140       1818\n",
       "Name: member_age_group, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['member_age'] = 2019 - df_clean.member_birth_year\n",
    "#replace zero value of member_birth_year which is now shown as 2019 by NaN\n",
    "df_clean.loc[df_clean['member_age'] == 2019, 'member_age'] = np.nan\n",
    "#Create bins based on the range of member_age\n",
    "bins = [10, 20, 30, 40, 50, 60, 70, 100, 140]\n",
    "df_clean['member_age_group'] = pd.cut(df_clean.member_age,bins, \\\n",
    "                                      labels=[\"10-20\",\"20-30\",\"30-40\",\"40-50\",\"50-60\",\"60-70\",\"70-100\",\"100-140\"] )\n",
    "print('Age Group: Number of rides')\n",
    "df_clean['member_age_group'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Duration in minutes\n",
      "count    3.015214e+06\n",
      "mean     1.466674e+01\n",
      "std      4.170826e+01\n",
      "min      1.016667e+00\n",
      "10%      3.833333e+00\n",
      "50%      9.283333e+00\n",
      "75%      1.455000e+01\n",
      "99%      1.011500e+02\n",
      "max      1.439483e+03\n",
      "Name: duration_sec, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Stats for Duration in minutes')\n",
    "print((df_clean['duration_sec']/60).describe(percentiles = [.1, .5,.75, .99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Distance in miles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    3.015214e+06\n",
       "mean     1.004740e+00\n",
       "std      6.493740e-01\n",
       "min      0.000000e+00\n",
       "10%      3.447281e-01\n",
       "50%      8.665367e-01\n",
       "75%      1.310504e+00\n",
       "99%      3.033850e+00\n",
       "max      4.316416e+01\n",
       "Name: minimum_distance_miles, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Stats for Distance in miles')\n",
    "df_clean['minimum_distance_miles'].describe(percentiles = [.1, .5,.75, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What the Statistics tells us from two variables `duration_sec`, and `minimum_distance_miles`?\n",
    "\n",
    ">- 75% of the rides took less than 15 minutes and travel less than 1.4 miles\n",
    ">- 99% of the rides took less than 102 minutes and travel less than 3.1 miles\n",
    ">- Average trip ride duration and distance are 12.35 minutes and 1 mile respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96486848"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Now let's remove outlies when we plot the distribution of each\n",
    "df_clean = df_clean[(df_clean.minimum_distance_miles <= 4)&(df_clean.duration_hr <= 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2994564, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Export the dataframe to .csv for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# As we already created new features, including distance and city we are exporting the data into csv\n",
    "df_clean.to_csv('master_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('master_data.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2994564, 33)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Here we show some sample of the dataset within the file size limit of Github (25Mb)\n",
    "df.sample(75000).to_csv('master_75k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Go to `2_FordGoBike_Exploratory_Part1` [here](https://github.com/abebe64/FordGoBike/2_FordGoBike_Exploratory_Part1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
